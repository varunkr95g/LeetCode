
Module 1: Examine components of a modern Data Warehouse
============================================================================

Describe modern data warehousing!!!

It consists of various relational and non-relaional sources, get fed to Azure Data Factory. From there, it's stored in Azure Data Lake Storage. Analysis can be hapen dirctly via Azure Synapse services or via Azure Databricks ( for further transformations) and then fed to Azure Synapse services. From Azure Synpase Services it's fed to Azure Analysis Services and then finally to Power BI for visualaisation.

Explore Azure data services for modern data warehousing!!!!

What is Azure Data Factory?????

Azure Data Factory is described as a data integration service. The purpose of Azure Data Factory is to retrieve data from one or more data sources, and convert it into a format that you process. For example, your data might contain dates and times formatted in different ways in different data sources. You can use Azure Data Factory to transform these items into a single uniform structure. 

What is Azure Data Lake Storage????

A data lake is a repository for large quantities of raw data. Because the data is raw and unprocessed, it's very fast to load and update, but the data hasn't been put into a structure suitable for efficient analysis.  It has the following characteristics:

Data Lake Storage organizes your files into directories and subdirectories for improved file organization.
Data Lake Storage supports the Portable Operating System Interface (POSIX) file and directory permissions to enable granular Role-Based Access Control (RBAC) on your data.
Azure Data Lake Storage is compatible with the Hadoop Distributed File System (HDFS).

What is Azure Databricks????

Azure Databricks is an Apache Spark environment running on Azure to provide big data processing, streaming, and machine learning. 

What is Azure Synapse Analytics???

Azure Synapse Analytics is an analytics engine. It's designed to process large amounts of data very quickly.Azure Synapse Analytics enables you to store the data you have read in and processed locally, within the service. 

Azure Synapse Analytics leverages a massively parallel processing (MPP) architecture. This architecture includes a control node and a pool of compute nodes.
( From my spark understanding control node is similar to deriver and compute node is similar to execuotrs.)

The Control node is the brain of the architecture. It's the front end that interacts with all applications. The MPP engine runs on the Control node to optimize and coordinate parallel queries. When you submit a processing request, the Control node transforms it into smaller requests that run against distinct subsets of the data in parallel.


The Compute nodes provide the computational power. The data to be processed is distributed evenly across the nodes. Users and applications send processing requests to the control node. The control node sends the queries to compute nodes, which run the queries over the portion of the data that they each hold. When each node has finished its processing, the results are sent back to the control node where they're combined into an overall result.

Azure Synapse Analytics supports two computational models: SQL pools and Spark pools.
In a SQL pool, each compute node uses an Azure SQL Database and Azure Storage to handle a portion of the data.
However, unlike an ordinary SQL Server database engine, Azure Synapse Analytics can receive data from a wide variety of sources.  To do this, Azure Synapse Analytics uses a technology named PolyBase. PolyBase enables you to retrieve data from relational and non-relational sources, such as delimited text files, Azure Blob Storage, and Azure Data Lake Storage.

Azure Synapse Analytics can consume a lot of resources. If you aren't planning on performing any processing for a while, you can pause the service. This action releases the resources in the pool to other users, and reduces your costs.

What is Azure Analysis Services?????

Azure Analysis Services enables you to build tabular models to support online analytical processing (OLAP) queries.

Compare Analysis Services with Synapse Analytics!!!!

Azure Analysis Services has significant functional overlap with Azure Synapse Analytics, but it's more suited for processing on a smaller scale.
Use Azure Synapse Analytics for:

Very high volumes of data (multi-terabyte to petabyte sized datasets).
Very complex queries and aggregations.
Complex ETL operations.
Low to mid concurrency (128 users or fewer).

Use Azure Analysis Services for:

Smaller volumes of data (a few terabytes).
High read concurrency (thousands of users).
Detailed analysis, and drilling into data, using functions in Power BI.
Rapid dashboard development from tabular data.

Using   Azure Analysis Services after the daa has been sufficiently ransformeed using Azure Synapse Analytics also is very efficient.

What is Azure HDInsight????

Azure HDInsight is a big data processing service, that provides the platform for technologies such as Spark in an Azure environment.


Module 2: Explore Large Scale Data Anaytics!!!
===============================================================================

Describe common practices for data loading!!!!!

What are the various methods for data loading?
1. Azure Data Factory
2. PolyBase
3. SQL Server Integration Services
4. Azure DataBricks

In a big data system, data ingestion has to be fast enough to capture the large quantities of data that may be heading your way, and have enough compute power to process this data in a timely manner.

Ingest data using Azure Data Factory!!!!!

Azure Data Factory is a data ingestion and transformation service that allows you to load raw data from many different sources, both on-premises and in the cloud. As it ingests the data, Data Factory can clean, transform, and restructure the data, before loading it into a repository such as a data warehouse

Data Factory provides an orchestration engine. Orchestration is the process of directing and controlling other services, and connecting them together, to allow data to flow between them. Data Factory uses orchestration to combine and automate sequences of tasks that use different services to perform complex operations.

Azure Data Factory uses a number of different resources: linked services, datasets, and pipelines. The following sections describe how Data Factory uses these resources.

Understand linked services!!!!

Data Factory moves data from a data source to a destination. A linked service provides the information needed for Data Factory to connect to a source or destination.

The information a linked service contains varies according to the resource. For example, to create a linked service for Azure Blob Storage, you provide information such as the name of the Azure subscription that owns the storage account, the name of the storage account, and the information necessary to authenticate against the storage account.

Understand datasets!!!!

A dataset in Azure Data Factory represents the data that you want to ingest (input) or store (output).

Understand pipelines!!!!

A pipeline is a logical grouping of activities that together perform a task.

Ingest data using PolyBase!!!!

PolyBase is a feature of SQL Server and Azure Synapse Analytics that enables you to run Transact-SQL queries that read data from external data sources. PolyBase makes these external data sources appear like tables in a SQL database.

Ingest data using SQL Server Integration Services!!!

SQL Server Integration Services (SSIS) is a platform for building enterprise-level data integration and data transformations solutions.

Describe data storage and processing!!!

What are the different methods for Data Processing:
1. Azure Synapse Analytics
2. Azure Databricks
3. Azure HDInsight
4. Azure Data Factory
5. Azure Data Lake 

Process data using Azure Synapse Analytics!!!

Azure Synapse Analytics is a generalized analytics service. You can use it to read data from many sources, process this data, generate various analyses and models, and save the results.

You can select between two technologies to process data:
Transact-SQL. This is the same dialect of SQL used by Azure SQL Database, with some extensions for reading data from external sources, such as databases, files, and Azure Data Lake storage
Spark. This is the same open-source technology used to power Azure Databricks. You write your analytical code using notebooks in a programming language such as C#, Scala, Python, or SQL

Process data using Azure Databricks!!!

Databricks can process data held in many different types of storage, including Azure Blob storage, Azure Data Lake Store, Hadoop storage, flat files, databases, and data warehouses.

Process data using Azure HDInsight!!!

Process data using Azure Data Factory!!!!

Using Azure Data Factory, you can create and schedule data-driven workflows (called pipelines) that can ingest data from the disparate data stores

Process data using Azure Data Lake!!

Azure Data Lake is a collection of analytics and storage services that you can combine to implement a big data solution. It comprises three main elements:

Data Lake Store
Data Lake Analytics
HDInsight

Data Lake Analytics uses a language called U-SQL. This is a hybrid language that takes features from both SQL and C#, and provides declarative and procedural capabilities that you can use to process data.

Explore Azure Synapse Analytics!!!!

Azure Synapse Analytics provides a suite of tools to analyze and process an organization's data. It incorporates SQL technologies, Transact-SQL query capabilities, and open-source Spark tools to enable you to quickly process very large amounts of data.

What are the components of Azure Synapse Analytics???

Azure Synapse is composed of the following elements:
1. Synapse SQL pool
2. Synapse Spark pool
3. Synapse Pipelines
4. Synapse Link
5. Synapse Studio

What are SQL pools???

Azure Synapse Analytics is designed to run queries over massive datasets. You can manually scale the SQL pool up to 60 nodes.  You can also pause a SQL pool if you don't require it for a while. Pausing releases the resources associated with the pool. You aren't charged for these resources until you manually resume the pool.

Use SQL pools in Synapse Analytics for the following scenarios:

Complex reporting. You can use the full power of Transact-SQL to run complex SQL statements that summarize and aggregate data.

Data ingestion. PolyBase enables you to retrieve data from many external sources and convert it into a tabular format. You can reformat this data and save it as tables and materialized views in Azure Synapse.

What are Spark pools?????

Spark pools in Synapse Analytics are especially suitable for the following scenarios

Data Engineering/Data Preparation. Apache Spark includes many language features to support preparation and processing of large volumes of data so that it can be made more valuable and then consumed by other services within Synapse Analytics. This is enabled through the Spark libraries that support processing and connectivity.

Machine Learning. Apache Spark comes with MLlib, a machine learning library built on top of Spark that you can use from a Spark pool in Synapse Analytics. Spark pools in Synapse Analytics also include Anaconda, a Python distribution with a variety of packages for data science including machine learning. When combined with built-in support for notebooks, you have an environment for creating machine learning applications.

What are Synapse pipelines????

A pipeline is a logical grouping of activities that together perform a task. 

What is Synapse Link???

Azure Synapse Link enables you to run near real-time analytics over operational data stored in Azure Cosmos DB. Synapse link uses a feature of Cosmos DB named Cosmos DB Analytical Store. Cosmos DB Analytical Store contains a copy of the data in a Cosmos DB container, but organized as a column store. Column stores are a more optimal format for running analytical workloads that need to aggregate data down a column rather than across a row, such as generating sum totals, averages, maximum or minimum values for a column.

Synapse link has a wide range of uses, including:

Supply chain analytics and forecasting.
Operational reporting. 
Batch data integration and orchestration.
Real-time personalization. 
IoT maintenance.



Module 3: Get Started Building With Power BI 
============================================================================

Use Power BI!!!

The common flow of activity looks like this:
Bring data into Power BI Desktop, and create a report.
Publish to the Power BI service, where you can create new visualizations or build dashboards.
Share dashboards with others, especially people who are on the go.
View and interact with shared dashboards and reports in Power BI Mobile apps.

Building blocks of Power BI!!!!

Here are the basic building blocks in Power BI:

Visualizations
Datasets
Reports
Dashboards
Tiles

Visualizations!!!!
A visualization (sometimes also referred to as a visual) is a visual representation of data. The goal of a visual is to present data in a way that provides context and insights, both of which would probably be difficult to discern from a raw table of numbers or text.

Datasets!!!
A dataset is a collection of data that Power BI uses to create its visualizations.

Reports!!!
In Power BI, a report is a collection of visualizations that appear together on one or more pages. 

Dashboards!!!
Power BI dashboard is a collection of visuals from a single page that you can share with others. A dashboard must fit on a single page, often called a canvas.

Tiles!!!
In Power BI, a tile is a single visualization on a dashboard.

Tour and use the Power BI service!!!!
 let's take a quick look at that first, and learn about an easy and popular way to quickly create visuals in Power BI: apps.
 
 An app is a collection of preset, ready-made visuals and reports that are shared with an entire organization. 


Module 4: Explore Fundamentals of Stream Processing
============================================================================
Understand batch and stream processing!!!

Batch processing, in which multiple data records are collected and stored before being processed together in a single operation.
Stream processing, in which a source of data is constantly monitored and processed in real time as new data events occur.

Understand batch processing!!

In batch processing, newly arriving data elements are collected into a group. The whole group is then processed at a future time as a batch

Advantages of batch processing include:

Large volumes of data can be processed at a convenient time.
It can be scheduled to run at a time when computers or systems might otherwise be idle, such as overnight, or during off-peak hours.

Disadvantages of batch processing include:

The time delay between ingesting the data and getting the results.
All of a batch job's input data must be ready before a batch can be processed. This means data must be carefully checked. Problems with data, errors, and program crashes that occur during batch jobs bring the whole process to a halt.

Understand stream processing!!!!

In stream processing, each new piece of data is processed when it arrives. Streaming data processing is beneficial in most scenarios where new, dynamic data is generated on a continual basis.

Real world examples of streaming data include:
A financial institution tracks changes in the stock market in real time
An online gaming company collects real-time data about player-game interactions, and feeds the data into its gaming platform
A system that monitors a building for smoke and heat needs to trigger alarms and unlock doors.

Understand differences between batch and streaming data!!!!

Data scope: Batch processing can process all the data in the dataset. Stream processing typically only has access to the most recent data received, or within a rolling time window (the last 30 seconds, for example).
Data size: Batch processing is suitable for handling large datasets efficiently. Stream processing is intended for individual records or micro batches consisting of few records.
Performance: The latency for batch processing is typically a few hours. Stream processing typically occurs immediately, with latency in the order of seconds or milliseconds. Latency is the time taken for the data to be received and processed.
Analysis: You typically use batch processing for performing complex analytics. Stream processing is used for simple response functions, aggregates, or calculations such as rolling averages.

Combine batch and stream processing!!!

Many large-scale analytics solutions include a mix of batch and stream processing, enabling both historical and real-time data analysis.

Explore common elements of stream processing architecture!!!!

A general architecture for stream processing!!

1. An event generates some data.
2. The generated data is written to a streaming source for processing. 
3. The event data is processed often by a perpetual query that operates on the event data to select data for specific types of events.
4. The results of the stream processing operation are written to an output (or sink), which may be a file, a database table, a real-time visual dashboard

Stream processing technologies!!!

The following two technologies are commonly used to process streaming data in Azure:
Azure Stream Analytics: A platform-as-a-service (PaaS) solution that you can use to define streaming jobs that ingest data from a streaming source, apply a perpetual query, and write the results to an output.
Spark Structured Streaming: An open-source library that enables you to develop complex streaming solutions on Apache Spark based services, including Azure Synapse Analytics, Azure Databricks, and Azure HDInsight.

Sources for stream processing:
1. Azure Event Hub
2.Azure IoT Hub
3.  Azure Data Lake Store Gen 2
4. Apache Kafka

Sinks for stream processing
1. Azure Event Hub
2.  Azure Data Lake Store Gen 2
3. Azure SQL Database or Azure Synapse Analytics, or Azure Databricks
4.Power BI

















