Q1. Dups with a few columns considered, for example

Dept Amount Name
1	1000	abc
1	1000	pqr
1	5000	ghy
2 	2000	vbg
2 	2000	bhg

select count(*),dept,amount from table group by dept,amount 
having count(*)>1;

Q2. 
Get 2nd maximum w.r.t each dept
Dept    Amount
A            1000
A            2000
A            3000
B            4000
B            2500
C            5000
C            4500

Output:
Dept    Amount
A           2000
B           2500
C           4500

select dept,amount
(
select dept,amount, row_number() over (partition by dept order by Amount) as from table
)cnt67
where cnt67.rn=2

Q3. SDC ( Slowly Changing Dimension )

Existing Record: table_existing
ID Name Designation Start_date End_date
1 AAA Engineer 2018–01–01 9999–12–31

New Record: new_table
ID Name Designation Start_date End_date
1 AAA Manager 2021–01–01 9999–12–31

Expected record ( post update)
ID Name Designation Start_date End_date
1 AAA Engineer 2018–01–01 2020–12–31
1 AAA Manager 2021–01–01 9999–12–31


First union and then below:

select ID, Name,Designation, Start_date
,COALESCE(date_sub(lag(Start_date) over (partition by ID order by Start_date desc),1),’9999–12–31') as End_date
from slowly_changing_dimension
Explanation:
Because of order by desc, the first row by default will have 9999–12–31 value as the End_date. For the other rows, the date prior to the start date becomes the end date.


Q3. Hive optimization techniques,

Q4. IF reduction in reducer size is mentioned, question about it's negative impacts

Q5. Map join

Q6. Analytic funcs like lead & lag

Q5. How do you determine the number of executors, executor memory in a spark action.

Q6. Narrow and wide tranformations

Q7.  Coalesce vs repartition

Q8. cache V/S persist

Q9. Date explosion in pyspark

Coalesce vs repartition
 Partitioning vs bucketing
 Spark performance management,tuning
 When do we introduce spark in scala?
 Udf using external jar
 Slowing changing dimensions 
 Fct vs dimension tables
 Hadoop cluster details
 Broadcast variables

Q10. How to determine by the SPLIT-BY Key in sqoop. Freeform SQL V/S bounday criteria 

Coding stuff:

Q1. 

arr=[1,1,2,3,4,5,5,5,5,5,5,5]

Find the element which occurs more than n/2 times.

Use a dictionary to get the frequency and return the key with value>n/2  

Q2.

arr = [2,8,4,3] -> false

IF there are 3 consecutive odd numbers in an array, return true else false 

arr=[1,3,5,8,6]-> true

Q3.

nums = [1,2,3,2] -> 4

Return the sum of unique elements in an array.

Use a dictionary to get the frequency and return the sum of keys with value=1



Q1. How are RDDs fault tolerant.

They maintain a lineage graph

Q2. What is the block size in Hadoop ? What's the impact on storage if the Block size is made 256 MB ?

No change in storage. 

Q3. 
What will be the output?

val dataSeq = Seq(("Java", 20000))
val rdd=sc.parallelize(dataSeq,8)

rdd.repartition(4)
rdd.getNumPartitions

8 -> RDDs are immutable, so no change will be done to rdd.


Q4.

nums = [3,2,1,5,6,4], k = 2

Find the kth largest number in an array


nums.sort(reverse=True)
return nums[k-1]

Q5. Difference b/w groupByKey and reduceByKey in RDD.

groupByKey shuffles the entire data over all partitions.
reduceByKey does the reduce operation within a partition. 
Then, the output of that partition is shuffled over the network.


Q6. Case 1 Hardware - 6 Nodes, and Each node 16 cores, 64 GB RAM

No. of Cores 		-> 5
No. of executors 	-> 18
Executor memory 	-> 20

No. of Cores is the no. of concurrent tasks. Always have this No. as 5.
So, 5 Cores per executor. 

5 * 3 = 15, so with 3 Executors per node, we'll consume 15 cores.
So, total No. of executors should be 6 ( No. of nodes) * 3 = 18.

Each node has 64 GB; 3 Executors per node, so each node should take 21 GB. But give 1GB
for other Hadoop processes, so, executor memory = 20 GB.

